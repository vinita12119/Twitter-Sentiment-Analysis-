{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter sentiment identification (Part -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "# To preprocess the data we need Tokenizer: here I have used nltk Tweet Tokenizer.\n",
    "# nltk is used for natural language processing and its Tweet tokenizer works specially on tweets data\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "# imported Sequential model to built layer-by-layer model. \n",
    "from keras.models import Sequential\n",
    "# imported Dense layer to build fully connected feedback network                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         to built layer-by-layer model. \n",
    "from keras.layers import Dense\n",
    "import numpy as np #importing numpy\n",
    "import pandas as pd #importing pandas\n",
    "np.random.seed(7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             [is, so, sad, for, my, APL, friend, ...]\n",
       "1            [I, missed, the, New, Moon, trailer, ...]\n",
       "2                      [omg, its, already, 7:30, :, O]\n",
       "3    [.., Omgaga, ., Im, sooo, im, gunna, CRy, ., I...\n",
       "4    [i, think, mi, bf, is, cheating, on, me, !, !,...\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step-1 Load data and tokenize it .\n",
    "# load data by pandas library's read_csv function while changin the encoding of file to read floadt and string without any errors(which were present in original encodingof the file)\n",
    "dataset2 = pd.read_csv(\"train_validation-1.csv\", sep= \",\",encoding = \"ISO-8859-1\")\n",
    "\n",
    "# tokenizer divides a string into substrings by splitting on the specified character or string (such as white space)\n",
    "# Tweettokenizer comes with few normalization features, strip_handles removes all the words starting with #symbol, @symbol and take out all the URLs\n",
    "tokenizer = TweetTokenizer(strip_handles=True,reduce_len=True)\n",
    "# All the tweets are stored in text variable\n",
    "text = dataset2.iloc[:,2]\n",
    "# here we are tokenizing it the tokenizer object we created earlier\n",
    "tokens = text.apply(tokenizer.tokenize)\n",
    "# check the data\n",
    "tokens.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline: 56.46%\n"
     ]
    }
   ],
   "source": [
    "# Baseline\n",
    "\n",
    "Baseline = (dataset2[dataset2['Sentiment']==1].shape[0] /dataset2.shape[0])*100\n",
    "print(\"\\nBaseline: %.2f%%\"%(Baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hmmm', '...', 'i', 'wonder', 'how', 'she', 'my', 'number', '@', '-', ')']\n",
      "['hmmm', '', 'i', 'wonder', 'how', 'she', 'my', 'number', '', '', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hmmm', 'i', 'wonder', 'how', 'she', 'my', 'number']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## STEP-2 : Data Preprocessing \n",
    "\n",
    "# Import re to use regular expressions\n",
    "import re\n",
    "# convert to all the strings to lower case\n",
    "for a,b in tokens.iteritems(): \n",
    "    tokens[a] =[i.lower() for i in b]\n",
    "print(tokens[9]) \n",
    "\n",
    "# Substitute all the special characters and number's string with empty string,\n",
    "for a,b in tokens.iteritems(): \n",
    "    tokens[a] =[re.sub(r'[^a-zA-z0-9\\s]','',i) for i in b]\n",
    "print(tokens[9])   \n",
    "\n",
    "\n",
    "#Remove special characters like empty string and standalone alphanumeric characters\n",
    "for a,b in tokens.iteritems(): \n",
    "    tokens[a] =[i for i in b if (i.isalpha())]\n",
    "tokens[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hmmm', 'wonder', 'number']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Stop Words like is, am , the , how , i etc.\n",
    "# nltk.download('stopwords') # use it when running for the first tome\n",
    "\n",
    "# Import stopwords list from nltk corpus module\n",
    "from nltk.corpus import stopwords\n",
    "# created unordered set of stopwords of english language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "allow_words = [\"not\", \"no\"]\n",
    "# match all the words in list of list with stop_words to remove them in final output\n",
    "for i,y in tokens.iteritems(): \n",
    "    tokens[i] = [x for x in y if x not in stop_words or x in allow_words ]\n",
    "        \n",
    "tokens[9]\n",
    "#reformed = [appos[word] if word in appos else word for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP - 3: Data Normalization\n",
    "# Lemmatization\n",
    "# nltk.download('wordnet') # use it when running for the first tome\n",
    "\n",
    "# We lemmatize the stings to only root words because in sentiment analysis root word matter most and \"Beautifully\" and \"Beautiful\" is only one word for it. \n",
    "#Import WordNet lemmatizer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#create WordNetLemmatizer object\n",
    "lemmat = WordNetLemmatizer()\n",
    "for i,lst in tokens.iteritems(): \n",
    "     tokens[i] = [lemmat.lemmatize(word) for word in lst]\n",
    "#print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99989,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the format from list of lists to lists of strings by joining all the tokenized strings to single strings sentence again.\n",
    "for i,lst in tokens.iteritems(): \n",
    "     tokens[i] = [' '.join(str(word) for word in lst) ]\n",
    "\n",
    "# convert the data type to dataframe\n",
    "df = pd.Series( (x[0] for x in tokens) )\n",
    "# Labels given in the original dataset file for Supervised learning \n",
    "yaxis= dataset2.iloc[:,1]\n",
    "yaxis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 48350)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# STEP -4: Feature extraction (Vectorization)\n",
    "# CountVectorizer allows to build a vocabulary (i.e dictionary) of known words and to encode new documents using that vocabulary.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# create the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "# tokenize here and fit the input dataframe on CountVectorizer model here.\n",
    "\n",
    "# In the vocabulary keys are the distinct words in the input dataframe and value \n",
    "# corresponding is the count of times each word appeared in that input dataframe\n",
    "vectorizer.fit(df)\n",
    "# \n",
    "#print(vectorizer.vocabulary_)\n",
    "# vectorized the input dataframe based on CountVectorizer model (encoding)  \n",
    "vector = vectorizer.transform(df)\n",
    "# convert scipy matrix to numpy array\n",
    "vec_array = vector.toarray()\n",
    "# Shape tell us the input of the first layer of model\n",
    "print(vector.shape)\n",
    "print(type(vec_array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split whole dataset into 80% training and 20% testing data. Here we using sklearn to the slpit and \n",
    "# train_test_split function takes input x-axis (which is vectorized feature) , y-axis (labels given in the question), \n",
    "# test_size to know conversion ratio of test data and random_state variable to make random number predictable and get almost same answer at every run. \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = vec_array[:80000],vec_array[80000:], yaxis[:80000],yaxis[80000:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining Sequential model instance \n",
    "#model.add(Dense(12,activation='relu'))\n",
    "\n",
    "model = Sequential()\n",
    "# first layer of the model with input size of number of columns of vectorized array and 20 is the number of neurons with 'relu' as activation function \n",
    "model.add(Dense(12,input_dim=len(vec_array[0]),activation='relu'))\n",
    "# 2 more hidden layers of model with 'relu' as activation function activation \n",
    "model.add(Dense(12,activation='relu'))\n",
    "model.add(Dense(12,activation='relu'))\n",
    "model.add(Dense(12,activation='relu'))\n",
    "# Output layer of model with only 1 column for output is only one of the two (1,0) and activation function is sigmoid\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model using binary_crossentropy for loss because it a binary classification and adam as optimizer with accuracy as metrics\n",
    "#optimizers = keras.optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "80000/80000 [==============================] - 101s 1ms/step - loss: 0.5211 - acc: 0.7418\n",
      "Epoch 2/5\n",
      "80000/80000 [==============================] - 102s 1ms/step - loss: 0.4116 - acc: 0.8093\n",
      "Epoch 3/5\n",
      "80000/80000 [==============================] - 102s 1ms/step - loss: 0.3379 - acc: 0.8486\n",
      "Epoch 4/5\n",
      "80000/80000 [==============================] - 100s 1ms/step - loss: 0.2789 - acc: 0.8777\n",
      "Epoch 5/5\n",
      "80000/80000 [==============================] - 112s 1ms/step - loss: 0.2315 - acc: 0.9001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x234cdf2d30>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the data by 5 epochs because accuracy changed very little after 3 iterations(epochs) and taking in batch size = 10 \n",
    "model.fit(x_train,y_train,epochs=5,batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000/80000 [==============================] - 48s 595us/step\n",
      "\n",
      "acc:92.94%\n"
     ]
    }
   ],
   "source": [
    "# Summarize data by evaluating and printing it accuracy on training data \n",
    "# Here accuracy is very high because model is trained with training set and we are evaluating accuracy of training set only.\n",
    "scores = model.evaluate(x_train,y_train)\n",
    "print(\"\\n%s:%.2f%%\"%(model.metrics_names[1],scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19989/19989 [==============================] - 11s 548us/step\n",
      "\n",
      "acc:73.49%\n"
     ]
    }
   ],
   "source": [
    "# Summarize data by evaluating and printing it accuracy on testing data \n",
    "\n",
    "scores = model.evaluate(x_test,y_test)\n",
    "print(\"\\n%s:%.2f%%\"%(model.metrics_names[1],scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
